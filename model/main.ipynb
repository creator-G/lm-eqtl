{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43e306-e5f1-4831-bb06-cecdb288fd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import pysam\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be2f56b-e9fb-4abc-9339-a335bc412afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from encoding_utils import sequence_encoders, sequence_utils\n",
    "\n",
    "import helpers.train_eval as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "from helpers.metrics import MaskedAccuracy\n",
    "from helpers.temperature_scaling import ModelWithTemperature\n",
    "\n",
    "\n",
    "from models.spec_dss import DSSResNet, DSSResNetEmb, SpecAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f455b8-053b-47d6-9206-d09fa0d11fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datadir = '/lustre/groups/epigenereg01/workspace/projects/vale/SysGen2023/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597bf5fb-c432-4eec-93da-17d6f10af4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seq_df, transform, max_augm_shift=0, \n",
    "                 mode='train'):\n",
    "\n",
    "        if input_params.dataset.endswith('.fa'):\n",
    "            self.fasta = pysam.FastaFile(input_params.dataset)\n",
    "        else:\n",
    "            self.fasta = None\n",
    "\n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "        self.max_augm_shift = max_augm_shift\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.seq_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.fasta:\n",
    "            seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        else:\n",
    "            seq = self.seq_df.iloc[idx].seq.upper()\n",
    "\n",
    "        shift = np.random.randint(self.max_augm_shift+1) #random shift at training, must be chunk_size-input_params.seq_len\n",
    "\n",
    "        seq = seq[shift:shift+input_params.seq_len] #shift the sequence and limit its size\n",
    "\n",
    "        seg_label = self.seq_df.iloc[idx].seg_label #label for segment-aware training\n",
    "\n",
    "        #for given genotype, randomly choose a haplotype for training/testing\n",
    "        if np.random.rand()>0.5:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R')\n",
    "        else:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf' and not input_params.test:\n",
    "        #    #select mask for the sequence depending on sequence coordinates w.r.t. contig\n",
    "        #    seg_name = self.seq_df.iloc[idx].seg_name\n",
    "        #    seq_mask = meta.loc[seg_name].MASK.values\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq, mask = seq_mask)\n",
    "        #else:\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "        \n",
    "        masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "\n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b717766-416d-4f5d-96d2-9e8e2051e704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "    #raise Exception('CUDA is not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1958cd83-205c-4a97-9852-08ab25b458ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f907e3-c581-41a5-80d1-034c93d2917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.dataset =  '/lustre/groups/epigenereg01/workspace/projects/vale/SysGen2023/data/new/phase3_top10/dataset.parquet'\n",
    "#input_params.species_list = datadir + 'fasta/240_species/240_species.txt'\n",
    "\n",
    "input_params.output_dir = './test'\n",
    "\n",
    "input_params.split_mask = False\n",
    "input_params.mask_rate = 0.2 #[0.012,0.2]#RAN #single float or 2 floats for reference and alternative\n",
    "input_params.masking = 'none' # stratified_maf or none\n",
    "\n",
    "input_params.test = False\n",
    "\n",
    "input_params.get_embeddings = False\n",
    "input_params.mask_at_test = True\n",
    "\n",
    "input_params.agnostic = True\n",
    "\n",
    "input_params.seq_len = 5000\n",
    "\n",
    "input_params.tot_epochs = 1\n",
    "input_params.fold = 0\n",
    "input_params.Nfolds = 5\n",
    "\n",
    "input_params.train_splits = 1\n",
    "\n",
    "input_params.save_at = [1]\n",
    "input_params.validate_every = 1\n",
    "\n",
    "input_params.d_model = 256\n",
    "input_params.n_layers = 16\n",
    "input_params.dropout = 0.\n",
    "\n",
    "input_params.batch_size = 16\n",
    "input_params.learning_rate = 1e-4\n",
    "input_params.weight_decay = 0\n",
    "\n",
    "#input_params.model_weight = 'test/epoch_1_weights_model'\n",
    "\n",
    "#input_params.optimizer_weight = '/lustre/groups/epigenereg01/workspace/projects/vale/SysGen2023/checkpoints/state_space/new/phase3_top10/agnostic_large/weights/epoch_20_weights_optimizer'\n",
    "#input_params.lr_sch_milestones = [500,1000]\n",
    "#input_params.lr_sch_gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043d233b-746b-41dd-ae97-a855bc0df81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_params.dataset.endswith('.fa'):\n",
    "    seq_df = pd.read_csv(input_params.dataset + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "elif input_params.dataset.endswith('.parquet'):\n",
    "    seq_df = pd.read_parquet(input_params.dataset).reset_index()\n",
    "    \n",
    "seq_df[['split','sample_id','seg_name']] =  seq_df['seq_name'].str.split(':',expand=True)\n",
    "\n",
    "#for each contig, mark first and lask chunks\n",
    "#seq_df[['split','sample_id','chrom','pos']] =  seq_df['seq_name'].str.split(':',expand=True)\n",
    "\n",
    "#seq_df.pos = seq_df.pos.astype(int)\n",
    "\n",
    "#df = seq_df.sort_values(by=['sample_id','chrom','pos']).copy()\n",
    "#df['last_chunk'] = ~df.duplicated(subset=['sample_id','chrom'],keep='last')\n",
    "#df['first_chunk'] = ~df.duplicated(subset=['sample_id','chrom'],keep='first')\n",
    "\n",
    "#seq_df = seq_df.merge(df[['seq_name','first_chunk','last_chunk']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b1e529d-cb8c-45cc-a2be-cc425f5c5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#\n",
    "#meta = pd.read_csv(os.path.dirname(input_params.dataset) + '/meta.csv.gz')[['GENE_ID','AF']]\n",
    "#meta['MAF'] = meta.AF.apply(lambda x: min(x,1-x)) #minor allele frequency\n",
    "#\n",
    "#meta['MAF_bin'] = np.digitize(meta.MAF, bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5], right=True) #bin indices for MAF\n",
    "#\n",
    "#meta.set_index('GENE_ID', inplace=True)\n",
    "#\n",
    "#def get_random_mask():\n",
    "#    '''\n",
    "#    randomly mask positions at mask_rate\n",
    "#    stratify by MAF distribution\n",
    "#    '''\n",
    "#    mask = np.zeros(len(meta.MAF_bin))\n",
    "#    masked_idx, _ = train_test_split(np.arange(len(meta.MAF_bin)), train_size=input_params.mask_rate,\n",
    "#                    shuffle=True, stratify=meta.MAF_bin)\n",
    "#    mask[masked_idx] = 1\n",
    "#    meta['MASK'] = mask\n",
    "#    \n",
    "#    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a3e0f3-6619-4616-bd26-a0ef7a5bbd8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not input_params.agnostic:\n",
    "    #for segment-aware model, assign a label to each segment\n",
    "    seg_name = seq_df.seq_name.apply(lambda x:':'.join(x.split(':')[2:]))\n",
    "    segment_encoding = seg_name.drop_duplicates().reset_index(drop=True)\n",
    "    segment_encoding = {seg_name:idx for idx,seg_name in segment_encoding.items()}\n",
    "    seq_df['seg_label'] = seg_name.map(segment_encoding)\n",
    "else:\n",
    "    seq_df['seg_label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0cc8291-8159-4926-847f-ac664f69a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_params.test:\n",
    "    seq_df = seq_df[seq_df.split=='test']\n",
    "else:\n",
    "    seq_df = seq_df[seq_df.split!='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4974f976-41ec-46c9-b543-c3bc70109eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(masked_sequence, species_label), targets_masked, targets, _ = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ce1c8b-d70c-4fe7-ac01-e86fd30a8dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2541394/512253626.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['train_fold'] = train_fold[:N_train]\n"
     ]
    }
   ],
   "source": [
    "test_df = None\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    #Train and Validate\n",
    "\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate = input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "\n",
    "    #N_train = int(len(seq_df)*(1-input_params.val_fraction))\n",
    "\n",
    "    if input_params.fold is not None:\n",
    "        \n",
    "        samples = seq_df.sample_id.unique()\n",
    "        val_samples = samples[input_params.fold::input_params.Nfolds] \n",
    "        \n",
    "        train_df = seq_df[~seq_df.sample_id.isin(val_samples)] \n",
    "        test_df = seq_df[seq_df.sample_id.isin(val_samples)]\n",
    "        \n",
    "        test_dataset = SeqDataset(test_df, transform = seq_transform, mode='eval')\n",
    "        test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 0, collate_fn = None, shuffle = False)\n",
    "    \n",
    "    else:\n",
    "        train_df = seq_df\n",
    "        \n",
    "        #train_df = seq_df[seq_df.split=='train']\n",
    "        #test_df = seq_df[seq_df.split=='val']\n",
    "  \n",
    "    N_train = len(train_df)\n",
    "\n",
    "    train_fold = np.repeat(list(range(input_params.train_splits)),repeats = N_train // input_params.train_splits + 1 )\n",
    "    train_df['train_fold'] = train_fold[:N_train]\n",
    "\n",
    "    train_dataset = SeqDataset(train_df, transform = seq_transform,  mode='train')\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = None, shuffle = False)\n",
    "\n",
    "\n",
    "elif input_params.get_embeddings:\n",
    "\n",
    "    if input_params.mask_at_test:\n",
    "        seq_transform = sequence_encoders.RollingMasker(mask_stride = 50, frame = 0)\n",
    "    else:\n",
    "        seq_transform = sequence_encoders.PlainOneHot(frame = 0, padding = 'none')\n",
    "\n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = None, shuffle = False)\n",
    "\n",
    "else:\n",
    "\n",
    "    #Test\n",
    "\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate=input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "\n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad86179c-11e5-4b3b-a389-b8ab1b00bdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seg_encoder = SpecAdd(embed = True, encoder = 'label', Nsegments=seq_df.seg_label.nunique(), d_model = input_params.d_model)\n",
    "\n",
    "model = DSSResNetEmb(d_input = 3, d_output = 3, d_model = input_params.d_model, n_layers = input_params.n_layers, \n",
    "                     dropout = input_params.dropout, embed_before = True, species_encoder = seg_encoder)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = input_params.learning_rate, weight_decay = input_params.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec227158-c405-4dc7-8f6a-38a83009d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 0\n",
    "\n",
    "if input_params.model_weight:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        #load on gpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight))\n",
    "    else:\n",
    "        #load on cpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight, map_location=torch.device('cpu')))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight, map_location=torch.device('cpu')))\n",
    "\n",
    "    last_epoch = int(input_params.model_weight.split('_')[-3]) #infer previous epoch from input_params.model_weight\n",
    "\n",
    "weights_dir = os.path.join(input_params.output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "\n",
    "if input_params.save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#        milestones=input_params.lr_sch_milestones, gamma=input_params.lr_sch_gamma, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f68efaa-d4a6-424f-a169-6d0d9046ec9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics_to_str(metrics):\n",
    "    loss, accuracy, masked_acc, masked_recall, masked_IQS = metrics\n",
    "    return f'loss: {loss:.4}, acc: {accuracy:.4}, masked acc: {masked_acc:.4}, {misc.print_class_recall(masked_recall, \"masked recall: \")}, masked IQS: {masked_IQS:.4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd086013-fd62-4964-8ccc-8b9d928d727d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Training...\n",
      "using train samples: [1, 20008]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 0/1000 [00:00<?, ?it/s]/home/icb/sergey.vilov/workspace/PRS/SysGen2023/model/models/dss.py:335: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:250.)\n",
      "  return einsum('chn,hnl->chl', W, S).float(), state                   # [C H L]\n",
      "/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
      "acc: 0.7674, masked recall: R=0.873;A=0.561;AVG=0.717, masked acc: 0.7887, masked IQS: 0.4511, loss: 0.4241:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                        | 481/1000 [06:51<07:13,  1.20it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6a190f9fc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1481, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1445, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/icb/sergey.vilov/miniconda3/envs/mlm/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#from helpers.misc import print    #print function that displays time\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    for epoch in range(last_epoch+1, input_params.tot_epochs+1):\n",
    "\n",
    "        print(f'EPOCH {epoch}: Training...')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf':\n",
    "\n",
    "        #    meta = get_random_mask()\n",
    "\n",
    "        train_dataset.seq_df = train_df[train_df.train_fold == (epoch-1) % input_params.train_splits]\n",
    "        print(f'using train samples: {list(train_dataset.seq_df.index[[0,-1]])}')\n",
    "\n",
    "        train_metrics = train_eval.model_train(model, optimizer, train_dataloader, device,\n",
    "                            silent = False)\n",
    "        \n",
    "        print(f'epoch {epoch} - train, {metrics_to_str(train_metrics)}')\n",
    "\n",
    "        if epoch in input_params.save_at: #save model weights\n",
    "\n",
    "            misc.save_model_weights(model, optimizer, weights_dir, epoch)\n",
    "\n",
    "        if test_df is not None  and ( epoch==input_params.tot_epochs or\n",
    "                            (input_params.validate_every and epoch%input_params.validate_every==0)):\n",
    "\n",
    "            print(f'EPOCH {epoch}: Validating...')\n",
    "\n",
    "            val_metrics, *_ =  train_eval.model_eval(model, optimizer, test_dataloader, device,\n",
    "                    silent = False)\n",
    "\n",
    "            print(f'epoch {epoch} - validation, {metrics_to_str(val_metrics)}')\n",
    "            \n",
    "        #lr_scheduler.step()\n",
    "else:\n",
    "\n",
    "    print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "    test_metrics, test_embeddings, motif_probas =  train_eval.model_eval(model, optimizer, test_dataloader, device, \n",
    "                                                          get_embeddings = input_params.get_embeddings, \n",
    "                                                          silent = False)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "    if input_params.get_embeddings:\n",
    "        \n",
    "        os.makedirs(input_params.output_dir, exist_ok = True)\n",
    "\n",
    "        with open(input_params.output_dir + '/embeddings.pickle', 'wb') as f:\n",
    "            #test_embeddings = np.vstack(test_embeddings)\n",
    "            #np.save(f,test_embeddings)\n",
    "            pickle.dump(test_embeddings,f)\n",
    "            #pickle.dump(seq_df.seq_name.tolist(),f)\n",
    "            \n",
    "print()\n",
    "print(f'peak GPU memory allocation: {round(torch.cuda.max_memory_allocated(device)/1024/1024)} Mb')\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
